{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPBAVH7dRg5p"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import os\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyQKAEEfRW2P"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "#tokenizer\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
        "#model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "EWFC_iFabfat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0oIQyPmRfON"
      },
      "outputs": [],
      "source": [
        "#Folders to store the tokenizers and the model\n",
        "#check if the directory exsist or not then create it\n",
        "\n",
        "model_folder = \"/content/drive/MyDrive/Grad_SA/Pre-training_from_checkpoint/tree_bank/models/model\"\n",
        "tokenizer_folder = \"/content/drive/MyDrive/Grad_SA/Pre-training_from_checkpoint/tree_bank/models/tokenizer\"\n",
        "isExist = os.path.exists(model_folder)\n",
        "\n",
        "if not isExist:\n",
        "\n",
        "  # Create a new directory because it does not exist\n",
        "  os.makedirs(model_folder)\n",
        "  print(\"The new directory is created!\")\n",
        "\n",
        "\n",
        "isExist = os.path.exists(tokenizer_folder)\n",
        "\n",
        "if not isExist:\n",
        "\n",
        "  # Create a new directory because it does not exist\n",
        "  os.makedirs(tokenizer_folder)\n",
        "  print(\"The new directory is created!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feFaP6ktTi_8"
      },
      "outputs": [],
      "source": [
        "#path of the full sudanese data \"in text \"\n",
        "mono_path = \"/content/drive/MyDrive/Grad_SA/col_test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QziZukm_TcUd"
      },
      "outputs": [],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "# setting up the data\n",
        "# step1: textdataset reads each line separately, tokenizes and truncates the lines to block_size, Adds special tokens.\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path= mono_path,\n",
        "    block_size=64,\n",
        ")\n",
        "# step2: Data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "\n",
        "#step3: setting up the trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    #save the model predictions and checkpoints\n",
        "    output_dir=\"/content/drive/MyDrive/Grad_SA/tree_bank roberta-more-pre-trained\",\n",
        "    overwrite_output_dir=True,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    seed=1,\n",
        "    #report the results and logs\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"Treebank RoBERTa more pre-training using Transfomers on PCM Data\",\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "#training\n",
        "trainer.train()\n",
        "#saving\n",
        "trainer.save_model(\"/content/drive/MyDrive/Grad_SA/tree_bank/roberta-retrained\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"/content/drive/MyDrive/Grad_SA/tree_bank/roberta-retrained\",\n",
        "    tokenizer=\"xlm-roberta-base\"\n",
        ")\n",
        "fill_mask(\"الدكتور قال بكرة ماف <mask>\")"
      ],
      "metadata": {
        "id": "37MhHkI5Tag0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VjDsqGlteW5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}